---
title: Observation processes for count data
format:
  gfm:
    code-fold: true
engine: jupyter
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.18.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# | label: setup
# | output: false
import jax.numpy as jnp
import numpy as np
import numpyro
import plotnine as p9
import pandas as pd
import warnings
from plotnine.exceptions import PlotnineWarning

warnings.filterwarnings("ignore", category=PlotnineWarning)

from _tutorial_theme import theme_tutorial
from pyrenew.observation import (
    CountBase,
    Counts,
    NegativeBinomialNoise,
    PoissonNoise,
)
from pyrenew.deterministic import DeterministicVariable, DeterministicPMF
from pyrenew import datasets
```

## Overview

Count observation processes model the lag between infections and an observed outcome such as hospital admissions, emergency department visits, confirmed cases, or deaths.
All count observation processes inherit from `CountBase`, which provides the core ascertainment x delay convolution pipeline.
PyRenew provides two built-in subclasses: `Counts` for aggregate daily counts and `CountsBySubpop` for subpopulation-level daily counts.
We demonstrates how to extend `CountBase` to create an observation process for weekly data.

### The generative model

In PyRenew, the observation process maps latent daily infections *forward* to observed data.
The generative direction is always:<br>
&emsp;&emsp; latent infections $\to$ ascertainment $\times$ delay convolution $\to$ predicted counts $\to$ noise $\to$ observed data.<br>
This forward direction is fundamental: the observation process transforms predictions to match the scale and resolution of the data, never the reverse.
During inference, the likelihood compares model predictions to observed data at the data's own resolution.

### The count observation equation

Count observation processes transform infections into predicted counts by applying an ascertainment rate and convolving with a delay distribution.

The predicted observations on day $t$ are:

$$\mu(t) = \alpha \sum_{s} I(t-s) \, \pi(s)$$

where:

- $I(t-s)$ is the number of incident (new) infections on day $t-s$ (i.e., $s$ days before day $t$)
- $\alpha$ is the rate of ascertained counts per infection (e.g., infection-to-hospital admission rate). This can model a mix of biological effects (e.g. some percentage of infections lead to hospital admissions, but not all) and reporting effects (e.g. some percentage of admissions that occur are reported, but not all).
- $\pi(s)$ is the delay distribution from infection to observation, conditional on an infection leading to an observation

Discrete observations are generated by sampling from a noise distribution - e.g. Poisson or negative binomial - to model reporting variability.
Poisson assumes variance equals the mean; negative binomial accommodates the overdispersion common in surveillance data.

## The CountBase Class

`CountBase` provides the core ascertainment $\times$ delay convolution operations that all subclasses inherit:

| Inherited method | What it does |
|---|---|
| `_predicted_obs(infections)` | Convolves infections with the delay PMF, scaled by the ascertainment rate. Returns daily predicted counts. |
| `validate()` | Validates the delay PMF, ascertainment rate, noise model, and optional day-of-week / right-truncation parameters. |
| `lookback_days()` | Returns `len(delay_pmf) - 1`: the number of initialization days needed. |
| `_apply_day_of_week(predicted, first_day_dow)` | Applies a multiplicative 7-day periodic pattern (optional). |
| `_apply_right_truncation(predicted, offset)` | Scales recent predictions for incomplete reporting (optional). |

The last two methods are optional adjustments that subclasses can apply in their `sample()` method. Day-of-week effects model systematic within-week reporting patterns (see [Day-of-Week Effects](day_of_week_effects.md)). Right-truncation adjustment accounts for incomplete reporting of recent observations (see [Right Truncation](right_truncation.md)).

Subclasses must implement three methods:

| Required method | Purpose |
|---|---|
| `infection_resolution()` | Return `"aggregate"` or `"subpop"` to tell the model which latent infections to route to this observation. |
| `sample(infections, ...)` | The main forward model: compute predictions, apply any transformations, then sample from the noise distribution. |
| `validate_data(n_total, n_subpops, ...)` | Validate observation data shapes before JAX tracing begins.

### Built-in subclasses

PyRenew provides two observation process classes:

- **`Counts`**: Aggregate daily counts. `infection_resolution()` returns `"aggregate"`. Accepts 1D infections of shape `(n_total,)`. Observations are on the shared dense time axis with NaN masking for initialization and missing data.
- **`CountsBySubpop`**: Subpopulation-level daily counts. `infection_resolution()` returns `"subpop"`. Accepts 2D infections of shape `(n_total, n_subpops)`. Uses sparse indexing via `times` and `subpop_indices`.


### Subclassing CountBase

The following code sketch outlines the functions that must be defined on a subclass of `CountBase`.

```python
from pyrenew.observation import CountBase
from pyrenew.observation.types import ObservationSample

class MyCustomCounts(CountBase):
    def __init__(self, name, ascertainment_rate_rv, delay_distribution_rv, noise):
        super().__init__(
            name=name,
            ascertainment_rate_rv=ascertainment_rate_rv,
            delay_distribution_rv=delay_distribution_rv,
            noise=noise,
        )

    def infection_resolution(self):
        return "aggregate"  # or "subpop"

    def validate_data(self, n_total, n_subpops, **kwargs):
        # Check observation data shapes
        ...

    def sample(self, infections, ...):
        predicted = self._predicted_obs(infections)  # inherited
        # ... transform predictions (e.g., aggregate to weekly) ...
        observed = self.noise.sample(name=..., predicted=..., obs=...)
        return ObservationSample(observed=observed, predicted=predicted)
```


## Using the `Counts` class for Hospital Admissions Data

For hospital admissions data, we construct a `Counts` observation process.
The delay is the key mechanism: infections from $s$ days ago ($I(t-s)$) contribute to today's expected hospital admissions ($\mu(t)$) weighted by the probability ($\pi(s)$) that an infection leads to hospitalization after exactly $s$ days. The convolution sums these contributions across all past days.

The process generates hospital admissions by sampling from a negative binomial distribution:

$$Y_t \sim \text{NegativeBinomial}(\text{mean} = \mu(t), \text{concentration} = \phi)$$

The concentration parameter $\phi$ (sometimes called $k$ or the dispersion parameter) controls overdispersion: as $\phi \to \infty$, the distribution approaches Poisson; smaller values allow greater overdispersion.

We use the negative binomial distribution because real-world hospital admission counts exhibit overdispersion - the variance exceeds the mean.
The Poisson distribution assumes variance equals the mean, which is too restrictive. The negative binomial adds an overdispersion term:

$$\text{Var}[Y_t] = \mu + \frac{\mu^2}{\phi}$$

In this example, we use fixed parameter values for illustration; in practice, these parameters would be estimated from data using weakly informative priors.

### Infection-to-hospitalization delay distribution

The delay distribution specifies the probability that an infected person is hospitalized $d$ days after infection, conditional on the infection leading to a hospitalization.
For example, if `hosp_delay_pmf[5] = 0.2`, then 20% of infections that result in hospitalization will appear as hospital admissions 5 days after infection.

We load a delay distribution from PyRenew's example datasets, compute summary statistics, and plot it. The distribution peaks around day 8-9 post-infection.

```{python}
# | label: delay-distribution
inf_hosp_int = datasets.load_infection_admission_interval()
hosp_delay_pmf = jnp.array(inf_hosp_int["probability_mass"].to_numpy())
delay_rv = DeterministicPMF("inf_to_hosp_delay", hosp_delay_pmf)

# Summary statistics
days = np.arange(len(hosp_delay_pmf))
mean_delay = float(np.sum(days * hosp_delay_pmf))
mode_delay = int(np.argmax(hosp_delay_pmf))
sd = float(np.sqrt(np.sum(days**2 * hosp_delay_pmf) - mean_delay**2))
print(
    f"mode delay: {mode_delay},  mean delay: {mean_delay:.1f},  sd: {sd:.1f}"
)
```

```{python}
# | label: plot-delay-distribution
delay_df = pd.DataFrame(
    {"days": days, "probability": np.array(hosp_delay_pmf)}
)

plot_delay = (
    p9.ggplot(delay_df, p9.aes(x="days", y="probability"))
    + p9.geom_col(fill="steelblue", alpha=0.7, color="black")
    + p9.geom_vline(
        xintercept=mode_delay, color="purple", linetype="solid", size=1
    )
    + p9.geom_vline(
        xintercept=mean_delay, color="red", linetype="dashed", size=1
    )
    + p9.labs(
        x="Days from infection to hospitalization",
        y="Probability",
        title="Infection-to-Hospitalization Delay Distribution",
    )
    + theme_tutorial
    + p9.annotate(
        "text",
        x=mode_delay + 8,
        y=max(delay_df["probability"]) * 0.95,
        label=f"Mode: {mode_delay} days",
        color="purple",
        size=10,
    )
    + p9.annotate(
        "text",
        x=mean_delay + 8,
        y=max(delay_df["probability"]) * 0.8,
        label=f"Mean: {mean_delay:.1f} days",
        color="red",
        size=10,
    )
)
plot_delay
```

### Defining a Counts observation process

A `Counts` object inherits the full convolution pipeline from `CountBase`. It takes the following arguments:

- **`name`**: unique, meaningful identifier for this observation process (e.g., `"hospital"`, `"deaths"`)
- **`ascertainment_rate_rv`**: the probability an infection results in an observation (e.g., IHR)
- **`delay_distribution_rv`**: delay distribution from infection to observation (PMF)
- **`noise`**: noise model (`PoissonNoise()` or `NegativeBinomialNoise(concentration_rv)`)

Observation processes are components in multi-signal models, where each signal must have a unique name. This name prefixes all numpyro sample sites (e.g., `"hospital"` creates sites `"hospital_obs"` and `"hospital_predicted"`), ensuring distinct identifiers in the inference trace.

For hospital admissions, the ascertainment rate is specifically called the infection-hospitalization rate (IHR).
In this example, the percentage of infections which lead to hospitalization is treated as a fixed value,
which will allow us to see how different values affect the model.
The concentration parameter for the negative binomial noise model is also fixed.
In practice, both of these parameters would be given a somewhat informative prior and then inferred.

```{python}
# | label: create-counts-process
# Infection-hospitalization ratio (1% of infections lead to hospitalization)
ihr_rv = DeterministicVariable("ihr", 0.01)

# Overdispersion parameter for negative binomial
concentration_rv = DeterministicVariable("concentration", 10.0)

# Create the observation process
hosp_process = Counts(
    name="hospital",
    ascertainment_rate_rv=ihr_rv,
    delay_distribution_rv=delay_rv,
    noise=NegativeBinomialNoise(concentration_rv),
)
```

### Timeline alignment and lookback period

The observation process convolves infections with a delay distribution, maintaining alignment between input and output: day $t$ in the output corresponds to day $t$ in the input.

Hospital admissions depend on infections from prior days. A delay PMF of length $L$ covers delays 0 to $L-1$, requiring $L-1$ days of prior infection history. The method `lookback_days()` returns $L-1$; the first valid observation day is at index `lookback_days()`. Earlier days are marked invalid.

```{python}
# | label: helper-function
print(f"Required lookback: {hosp_process.lookback_days()} days")


def first_valid_observation_day(obs_process) -> int:
    """Return the first day index with complete infection history for convolution."""
    return obs_process.lookback_days()
```

## Simulations

In real-world inference, incident infections are a *latent* (unobserved) quantity and must be estimated from observed data.
To simulate the observation process we specify infections directly to show how they produce observed counts through convolution and sampling.

### Single-day infection spike

To demonstrate how a `Counts` observation process works, we examine how infections occurring on a single day result in observed hospital admissions.

```{python}
# | label: simulate-spike
n_days = 100
day_one = first_valid_observation_day(hosp_process)

# Create infections with a spike
infection_spike_day = day_one + 10
infections = jnp.zeros(n_days)
infections = infections.at[infection_spike_day].set(2000)
```

We plot the infections starting from day_one (the first valid observation day, after the lookback period).

```{python}
# | label: plot-infections
# Plot relative to first valid observation day
n_plot_days = n_days - day_one
rel_spike_day = infection_spike_day - day_one

infections_df = pd.DataFrame(
    {
        "day": np.arange(n_plot_days),
        "count": np.array(infections[day_one:]),
    }
)

max_infection_count = float(jnp.max(infections[day_one:]))

plot_infections = (
    p9.ggplot(infections_df, p9.aes(x="day", y="count"))
    + p9.geom_line(color="darkblue", size=1)
    + p9.geom_point(color="darkblue", size=2)
    + p9.geom_vline(
        xintercept=rel_spike_day,
        color="darkred",
        linetype="dashed",
        alpha=0.5,
    )
    + p9.labs(x="Day", y="Daily Infections", title="Infections (Input)")
    + theme_tutorial
    + p9.annotate(
        "text",
        x=rel_spike_day + 2,
        y=max_infection_count * 0.9,
        label=f"Infection spike (day {rel_spike_day})",
        color="darkred",
        size=10,
    )
)
plot_infections
```

Because all infections occur on a single day, this allows us to see how one day's worth of infections result in hospital admissions spread over subsequent days according to the delay distribution.

### Predicted admissions without observation noise

First, we compute the predicted admissions from the convolution alone, without observation noise. This is the mean of the distribution from which samples are drawn.

```{python}
# | label: predicted-no-noise
# Compute predicted admissions (convolution only, no observation noise)
from pyrenew.convolve import compute_delay_ascertained_incidence

# Scale infections by IHR (ascertainment rate)
infections_scaled = infections * float(ihr_rv.sample())
predicted_admissions, offset = compute_delay_ascertained_incidence(
    p_observed_given_incident=1.0,
    latent_incidence=infections_scaled,
    delay_incidence_to_observation_pmf=hosp_delay_pmf,
    pad=True,
)
```

```{python}
# | label: plot-predicted-no-noise
# Relative peak day for plotting
peak_day = rel_spike_day + mode_delay

# Plot predicted admissions (x-axis: day_one = first valid observation day)
predicted_df = pd.DataFrame(
    {
        "day": np.arange(n_plot_days),
        "admissions": np.array(predicted_admissions[day_one:]),
    }
)

max_predicted = float(predicted_df["admissions"].max())
plot_predicted = (
    p9.ggplot(predicted_df, p9.aes(x="day", y="admissions"))
    + p9.geom_line(color="purple", size=1)
    + p9.geom_point(color="purple", size=1.5)
    + p9.geom_vline(
        xintercept=rel_spike_day,
        color="darkred",
        linetype="dashed",
        alpha=0.5,
    )
    + p9.geom_vline(
        xintercept=peak_day,
        color="purple",
        linetype="dashed",
        alpha=0.5,
    )
    + p9.labs(
        x="Day",
        y="Hospital Admissions",
        title="Predicted Hospital Admissions (Deterministic)",
    )
    + theme_tutorial
    + p9.annotate(
        "text",
        x=rel_spike_day,
        y=max_predicted * 1.05,
        label=f"Infection spike\n(day {rel_spike_day})",
        color="darkred",
        size=9,
        ha="center",
    )
    + p9.annotate(
        "text",
        x=peak_day,
        y=max_predicted * 1.05,
        label=f"Peak\n(day {peak_day})",
        color="purple",
        size=9,
        ha="center",
    )
)
plot_predicted
```

The predicted admissions mirror the delay distribution, shifted by the infection spike day and scaled by the IHR.


### Observation noise (Negative Binomial)

The negative binomial distribution adds stochastic variation. Sampling multiple times from the same infections shows the range of possible observations:

```{python}
# | label: sample-realizations
# Sample 50 realizations of hospital admissions from the same infection spike
n_samples = 50
samples_list = []

for seed in range(n_samples):
    with numpyro.handlers.seed(rng_seed=seed):
        hosp_sample = hosp_process.sample(infections=infections, obs=None)

    # Slice from day_one to align with valid observation period
    for i, val in enumerate(hosp_sample.observed[day_one:]):
        samples_list.append(
            {
                "day": i,
                "admissions": float(val),
                "sample": seed,
                "type": "sampled",
            }
        )

# Add predicted values
for i, val in enumerate(predicted_admissions[day_one:]):
    samples_list.append(
        {
            "day": i,
            "admissions": float(val),
            "sample": -1,
            "type": "predicted",
        }
    )
```

```{python}
# | label: plot-realizations
samples_df = pd.DataFrame(samples_list)
sampled_df = samples_df[samples_df["type"] == "sampled"]
predicted_noise_df = samples_df[samples_df["type"] == "predicted"]

# Separate one sample to highlight
highlight_sample = 0
other_samples_df = sampled_df[sampled_df["sample"] != highlight_sample]
highlight_df = sampled_df[sampled_df["sample"] == highlight_sample]

plot_50_samples = (
    p9.ggplot()
    + p9.geom_line(
        p9.aes(x="day", y="admissions", group="sample"),
        data=other_samples_df,
        color="orange",
        alpha=0.15,
        size=0.5,
    )
    + p9.geom_line(
        p9.aes(x="day", y="admissions"),
        data=highlight_df,
        color="steelblue",
        size=1,
    )
    + p9.geom_line(
        p9.aes(x="day", y="admissions"),
        data=predicted_noise_df,
        color="darkred",
        size=1.2,
    )
    + p9.geom_vline(
        xintercept=rel_spike_day,
        color="darkblue",
        linetype="dashed",
        alpha=0.5,
    )
    + p9.labs(
        x="Day",
        y="Hospital Admissions",
        title=f"Observation Noise: {n_samples} Samples from Same Infections",
        subtitle="Blue: one realization | Orange: other samples | Dark red: predicted",
    )
    + theme_tutorial
)
plot_50_samples
```

```{python}
# | label: timeline-stats
# Print timeline statistics
print("Timeline Analysis:")
print(
    f"  Infection spike on day {rel_spike_day}: {infections[infection_spike_day]:.0f} people"
)
print(f"  Mode delay from infection to hospitalization: {mode_delay} days")
print(
    f"  Predicted hospitalization peak: day {rel_spike_day + mode_delay} (= {rel_spike_day} + {mode_delay})"
)
```

### Effect of the ascertainment rate

The ascertainment rate (here, the infection-hospitalization rate or IHR) directly scales the number of predicted hospital admissions.
We compare two contrasting IHR values: **0.5%** and **2.5%**.

```{python}
# | label: compare-ihr
# Two contrasting IHR values
ihr_values = [0.005, 0.025]
peak_value = 3000  # Peak infections
infections_decay = peak_value * jnp.exp(-jnp.arange(n_days) / 20.0)

# Compute predicted hospital admissions (no noise) for each IHR
results_list = []
for ihr_val in ihr_values:
    infections_scaled = infections_decay * ihr_val
    predicted_hosp, _ = compute_delay_ascertained_incidence(
        p_observed_given_incident=1.0,
        latent_incidence=infections_scaled,
        delay_incidence_to_observation_pmf=hosp_delay_pmf,
        pad=True,
    )

    for i, admit in enumerate(predicted_hosp[day_one:]):
        results_list.append(
            {
                "day": i,
                "admissions": float(admit),
                "IHR": f"IHR = {ihr_val:.1%}",
            }
        )
```

```{python}
# | label: plot-ihr-comparisons
results_df = pd.DataFrame(results_list)

plot_ihr = (
    p9.ggplot(results_df, p9.aes(x="day", y="admissions", color="IHR"))
    + p9.geom_line(size=1)
    + p9.scale_color_manual(values=["steelblue", "darkred"])
    + p9.labs(
        x="Day",
        y="Predicted Hospital Admissions",
        title="Effect of IHR on Predicted Hospital Admissions",
        color="Infection-Hospitalization\nrate",
    )
    + theme_tutorial
)
plot_ihr
```

### Negative binomial concentration parameter

The concentration parameter $\phi$ controls overdispersion:

- Higher $\phi$ → less overdispersion (approaches Poisson)
- Lower $\phi$ → more overdispersion (noisier data)

We compare three concentration values spanning two orders of magnitude:

- $\phi = 1$: high overdispersion (noisy)
- $\phi = 10$: moderate overdispersion
- $\phi = 100$: nearly Poisson (minimal noise)

```{python}
# | label: concentration-comparisons
# Use constant infections
peak_value = 2000
infections_constant = peak_value * jnp.ones(n_days)

# Concentration values spanning two orders of magnitude
concentration_values = [1.0, 10.0, 100.0]
n_replicates = 10

# Collect results
conc_results = []
for conc_val in concentration_values:
    conc_rv_temp = DeterministicVariable("conc", conc_val)
    process_temp = Counts(
        name="hospital",
        ascertainment_rate_rv=ihr_rv,
        delay_distribution_rv=delay_rv,
        noise=NegativeBinomialNoise(conc_rv_temp),
    )

    for seed in range(n_replicates):
        with numpyro.handlers.seed(rng_seed=seed):
            hosp_temp = process_temp.sample(
                infections=infections_constant,
                obs=None,
            )

        # Slice from day_one to align with valid observation period
        for i, admit in enumerate(hosp_temp.observed[day_one:]):
            conc_results.append(
                {
                    "day": i,
                    "admissions": float(admit),
                    "concentration": f"$\\phi$ = {int(conc_val)}",
                    "replicate": seed,
                }
            )
```

```{python}
# | label: plot-concentration-comparisons
conc_df = pd.DataFrame(conc_results)

# Convert to ordered categorical
conc_df["concentration"] = pd.Categorical(
    conc_df["concentration"],
    categories=["$\\phi$ = 1", "$\\phi$ = 10", "$\\phi$ = 100"],
    ordered=True,
)

plot_concentration = (
    p9.ggplot(conc_df, p9.aes(x="day", y="admissions", group="replicate"))
    + p9.geom_line(alpha=0.5, size=0.8, color="steelblue")
    + p9.facet_wrap("~ concentration", ncol=3)
    + p9.labs(
        x="Day",
        y="Hospital Admissions",
        title="Effect of Negative Binomial Concentration Parameter on Variability",
    )
    + theme_tutorial
)
plot_concentration
```

### Swapping noise models

To use Poisson noise instead of negative binomial, change the noise model:

```{python}
# | label: poisson-noise
hosp_process_poisson = Counts(
    name="hospital",
    ascertainment_rate_rv=ihr_rv,
    delay_distribution_rv=delay_rv,
    noise=PoissonNoise(),
)

with numpyro.handlers.seed(rng_seed=42):
    poisson_result = hosp_process_poisson.sample(
        infections=infections,
        obs=None,
    )

print(
    f"Sampled {len(poisson_result.observed)} days of hospital admissions with Poisson noise"
)
```

To compare Poisson noise directly against negative binomial, we plot 10 replicates from three noise models side by side using the same constant infection input. The shared y-axis makes the difference in variability immediately visible: Poisson ($\text{Var} = \mu$) is the tightest, negative binomial with $\phi = 100$ is nearly identical, and $\phi = 10$ shows noticeably more spread.

```{python}
# | label: poisson-realizations
noise_comparison = []
noise_configs = [
    ("Poisson", PoissonNoise()),
    (
        "NegBin $\\phi$=100",
        NegativeBinomialNoise(DeterministicVariable("c100", 100.0)),
    ),
    (
        "NegBin $\\phi$=10",
        NegativeBinomialNoise(DeterministicVariable("c10", 10.0)),
    ),
]

for label, noise_model in noise_configs:
    process_tmp = Counts(
        name="hospital",
        ascertainment_rate_rv=ihr_rv,
        delay_distribution_rv=delay_rv,
        noise=noise_model,
    )
    for seed in range(10):
        with numpyro.handlers.seed(rng_seed=seed):
            result_tmp = process_tmp.sample(
                infections=infections_constant, obs=None
            )
        for i, admit in enumerate(result_tmp.observed[day_one:]):
            noise_comparison.append(
                {
                    "day": i,
                    "admissions": float(admit),
                    "noise": label,
                    "replicate": seed,
                }
            )

noise_df = pd.DataFrame(noise_comparison)
noise_df["noise"] = pd.Categorical(
    noise_df["noise"],
    categories=["Poisson", "NegBin $\\phi$=100", "NegBin $\\phi$=10"],
    ordered=True,
)

(
    p9.ggplot(noise_df, p9.aes(x="day", y="admissions", group="replicate"))
    + p9.geom_line(alpha=0.5, size=0.8, color="steelblue")
    + p9.facet_wrap("~ noise", ncol=3)
    + p9.labs(
        x="Day",
        y="Hospital Admissions",
        title="Noise Model Comparison: Poisson vs. Negative Binomial",
    )
    + theme_tutorial
)
```

## Weekly Observations with WeeklyCounts

Some surveillance signals are reported at coarser temporal resolution than daily. For example, NHSN hospital admissions are now reported as weekly (MMWR epiweek) totals rather than daily counts.

The correct approach is to aggregate *predictions* up to the observation's temporal resolution, not to disaggregate observations down to daily.
Disaggregating weekly counts to daily values would fabricate within-week timing information that does not exist in the data.
Instead, the latent model produces daily predictions via `_predicted_obs()` (inherited from `CountBase`), and the observation process sums them into weekly totals.
The likelihood then evaluates at the weekly resolution - comparing weekly predicted totals to weekly observed totals.
This preserves the generative model's causal direction: latent daily infections flow forward through the observation process to produce predictions at whatever resolution the data requires.

The predicted weekly admissions for epiweek $w$ are:

$$\mu_w = \sum_{d \in w} \mu(d)$$

where $\mu(d)$ is the daily predicted count. Observations are weekly totals with negative binomial noise:

$$Y_w \sim \text{NegativeBinomial}(\text{mean} = \mu_w, \text{concentration} = \phi)$$

Weekly aggregation naturally reduces variance relative to daily counts, so weekly observations typically use a higher concentration parameter (less overdispersion) than daily observations.
The choice of $\phi$ at each resolution reflects prior knowledge about the noise structure of the data.
Daily counts are subject to day-to-day reporting irregularities: staffing variation, batch reporting, and weekday/weekend effects all introduce overdispersion beyond what the Poisson model predicts.
A moderate $\phi$ (e.g., 10) captures this extra daily noise.
Weekly totals average over these within-week fluctuations, so the remaining noise after aggregation is closer to Poisson.
A high $\phi$ (e.g., 100) is appropriate because most of the reporting-driven overdispersion has been smoothed out by summing over 7 days.
In practice, both concentration parameters would be given informative priors and estimated from data, but the prior for the weekly $\phi$ should be centered higher than the prior for the daily $\phi$.

Day-of-week effects and right-truncation are not applicable to weekly data: weekly aggregation absorbs within-week patterns and mitigates reporting delays.

### Implementing the WeeklyCounts class

```{python}
# | label: weekly-counts-class
from jax.typing import ArrayLike
from pyrenew.observation import CountBase
from pyrenew.observation.noise import CountNoise
from pyrenew.observation.types import ObservationSample
from pyrenew.metaclass import RandomVariable
from pyrenew.time import daily_to_mmwr_epiweekly


class WeeklyCounts(CountBase):
    """Weekly (MMWR epiweek) aggregate count observation process."""

    def __init__(
        self,
        name: str,
        ascertainment_rate_rv: RandomVariable,
        delay_distribution_rv: RandomVariable,
        noise: CountNoise,
    ) -> None:
        """
        Initialize weekly count observation process.

        Parameters
        ----------
        name : str
            Unique name for this observation process.
        ascertainment_rate_rv : RandomVariable
            Ascertainment rate in [0, 1] (e.g., IHR).
        delay_distribution_rv : RandomVariable
            Delay distribution PMF (must sum to ~1.0).
        noise : CountNoise
            Noise model for weekly count observations.
        """
        super().__init__(
            name=name,
            ascertainment_rate_rv=ascertainment_rate_rv,
            delay_distribution_rv=delay_distribution_rv,
            noise=noise,
        )

    def infection_resolution(self) -> str:
        """Return 'aggregate' for jurisdiction-level observations."""
        return "aggregate"

    def validate_data(
        self,
        n_total: int,
        n_subpops: int,
        first_day_dow: int | None = None,
        week_indices: ArrayLike | None = None,
        obs: ArrayLike | None = None,
        **kwargs,
    ) -> None:
        """
        Validate weekly observation data.

        Parameters
        ----------
        n_total : int
            Total time steps on the shared daily axis.
        n_subpops : int
            Number of subpopulations (unused).
        first_day_dow : int | None
            Day of the week for element 0 of the shared time axis.
        week_indices : ArrayLike | None
            Indices into the weekly-aggregated predictions array.
        obs : ArrayLike | None
            Weekly observed counts.
        **kwargs
            Additional keyword arguments (ignored).
        """
        if obs is not None and week_indices is not None:
            obs = jnp.asarray(obs)
            week_indices = jnp.asarray(week_indices)
            if obs.shape != week_indices.shape:
                raise ValueError(
                    f"Observation '{self.name}': obs shape {obs.shape} "
                    f"must match week_indices shape {week_indices.shape}"
                )

    def sample(
        self,
        infections: ArrayLike,
        first_day_dow: int,
        week_indices: ArrayLike,
        obs: ArrayLike | None = None,
    ) -> ObservationSample:
        """
        Sample weekly aggregated counts.

        Parameters
        ----------
        infections : ArrayLike
            Daily aggregate infections, shape (n_total,).
        first_day_dow : int
            ISO day-of-week for element 0 of the shared time axis
            (0=Monday, 6=Sunday).
        week_indices : ArrayLike
            Indices into the weekly predictions array identifying
            which weeks have observations.
        obs : ArrayLike | None
            Weekly observed counts, shape (n_obs_weeks,).
            None for prior predictive sampling.

        Returns
        -------
        ObservationSample
            Named tuple with observed (weekly) and predicted (daily).
        """
        daily_predicted = self._predicted_obs(infections)
        self._deterministic("predicted_daily", daily_predicted)

        weekly_predicted = daily_to_mmwr_epiweekly(
            daily_predicted, input_data_first_dow=first_day_dow
        )
        self._deterministic("predicted_weekly", weekly_predicted)

        predicted_at_obs = weekly_predicted[week_indices]

        observed = self.noise.sample(
            name=self._sample_site_name("obs"),
            predicted=predicted_at_obs,
            obs=obs,
        )

        return ObservationSample(observed=observed, predicted=daily_predicted)
```

Key design choices:

- **No day-of-week or right-truncation**: The constructor passes neither `day_of_week_rv` nor `right_truncation_rv` to `CountBase`. Weekly aggregation absorbs within-week patterns and mitigates reporting delays.
- **`week_indices`**: Maps observed weeks to positions in the aggregated predictions. This handles partial weeks at the start/end of the time series and allows for missing weeks.
- **Two deterministic sites**: `predicted_daily` (full daily time series) and `predicted_weekly` (aggregated epiweek totals) are both recorded for posterior analysis.

### Configuring a weekly hospital admissions process

```{python}
# | label: create-weekly-process
weekly_ihr_rv = DeterministicVariable("weekly_ihr", 0.01)
weekly_concentration_rv = DeterministicVariable("weekly_concentration", 100.0)

weekly_hosp_process = WeeklyCounts(
    name="hospital_weekly",
    ascertainment_rate_rv=weekly_ihr_rv,
    delay_distribution_rv=delay_rv,
    noise=NegativeBinomialNoise(weekly_concentration_rv),
)

print(f"Required lookback: {weekly_hosp_process.lookback_days()} days")
```

### Comparing daily and weekly observations from the same infections

Using the exponentially decaying infection curve from earlier, we can see how the same underlying epidemic produces different observations at daily vs. weekly resolution.

```{python}
# | label: weekly-simulate
import datetime as dt

peak_value = 3000
infections_decay = peak_value * jnp.exp(-jnp.arange(n_days) / 20.0)

# The shared time axis starts on a Sunday (2023-01-01 was a Sunday = ISO dow 6)
first_dow = 6

# Compute weekly predictions to determine valid week indices
with numpyro.handlers.seed(rng_seed=0):
    daily_predicted = weekly_hosp_process._predicted_obs(infections_decay)

weekly_predicted = daily_to_mmwr_epiweekly(
    daily_predicted, input_data_first_dow=first_dow
)
n_valid_weeks = int(jnp.sum(~jnp.isnan(weekly_predicted)))
n_total_weeks = len(weekly_predicted)

# Use all valid (non-NaN) weeks
all_week_indices = jnp.arange(n_total_weeks)
valid_mask = ~jnp.isnan(weekly_predicted)
week_indices = all_week_indices[valid_mask]

print(
    f"Total weeks: {n_total_weeks}, "
    f"valid weeks (after lookback): {n_valid_weeks}"
)
```

```{python}
# | label: weekly-daily-comparison
# Sample daily observations (using existing Counts process)
daily_process = Counts(
    name="hospital_daily",
    ascertainment_rate_rv=weekly_ihr_rv,
    delay_distribution_rv=delay_rv,
    noise=NegativeBinomialNoise(DeterministicVariable("conc_daily", 10.0)),
)

# Collect daily and weekly samples
comparison_list = []

for seed in range(50):
    with numpyro.handlers.seed(rng_seed=seed):
        daily_result = daily_process.sample(
            infections=infections_decay, obs=None
        )

    for i, val in enumerate(daily_result.observed[day_one:]):
        comparison_list.append(
            {
                "time": i,
                "admissions": float(val),
                "resolution": "Daily ($\\phi$=10)",
                "replicate": seed,
            }
        )

    with numpyro.handlers.seed(rng_seed=seed):
        weekly_result = weekly_hosp_process.sample(
            infections=infections_decay,
            first_day_dow=first_dow,
            week_indices=week_indices,
            obs=None,
        )

    for j, (wi, val) in enumerate(zip(week_indices, weekly_result.observed)):
        comparison_list.append(
            {
                "time": int(wi) * 7 + 3 - day_one,
                "admissions": float(val),
                "resolution": "Weekly ($\\phi$=100)",
                "replicate": seed,
            }
        )

comparison_df = pd.DataFrame(comparison_list)
```

```{python}
# | label: plot-weekly-daily-comparison
daily_comp = comparison_df[comparison_df["resolution"] == "Daily ($\\phi$=10)"]
weekly_comp = comparison_df[
    comparison_df["resolution"] == "Weekly ($\\phi$=100)"
]

(
    p9.ggplot()
    + p9.geom_line(
        p9.aes(x="time", y="admissions", group="replicate"),
        data=daily_comp,
        color="steelblue",
        alpha=0.2,
        size=0.5,
    )
    + p9.geom_jitter(
        p9.aes(x="time", y="admissions", group="replicate"),
        data=weekly_comp,
        color="orange",
        alpha=0.6,
        size=2,
        width=1.2,
        height=0,
    )
    + p9.labs(
        x="Day (relative to first valid observation day)",
        y="Hospital Admissions",
        title="Daily vs. Weekly: 50 Sample Observations from Same Infections ",
        subtitle="Blue lines: daily ($\\phi$=10) | Orange points: weekly totals ($\\phi$=100)",
    )
    + theme_tutorial
)
```

Weekly aggregation collapses seven daily values into a single total per epiweek.
The weekly points may appear more dispersed than the daily lines, even though the weekly process uses a much higher concentration parameter ($\phi = 100$ vs. $\phi = 10$).
This is not a modeling error.
The negative binomial variance is $\text{Var}[Y] = \mu + \mu^2 / \phi$.
Weekly totals have means roughly 7 times larger than daily means ($\mu_w \approx 7 \mu_d$), so the quadratic term $\mu_w^2 / \phi$ grows with the square of the mean.
Even with $\phi = 100$, the absolute spread of the weekly distribution is wider than the daily distribution with $\phi = 10$, because the weekly mean is so much larger.
In relative terms (coefficient of variation), the weekly observations are tighter, which is why weekly data is often considered less noisy for inference.

In a multi-signal model, pairing weekly hospital admissions with a daily signal (such as ED visits) allows the daily signal to resolve within-week dynamics that the weekly signal cannot capture.
