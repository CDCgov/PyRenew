---
title: "Observation processes for continuous measurements"
format: gfm
engine: jupyter
---

This tutorial demonstrates how to use the `Measurements` observation process to model continuous measurement data. We first explain the general framework, then illustrate with a wastewater viral concentration example.

```{python}
# | label: setup
# | output: false
import jax
import jax.numpy as jnp
import numpy as np
import numpyro
import matplotlib.pyplot as plt

import numpyro.distributions as dist

from pyrenew.observation import Measurements, HierarchicalNormalNoise
from pyrenew.randomvariable import DistributionalVariable
from pyrenew.deterministic import DeterministicVariable, DeterministicPMF
```

# Part 1: The Measurements Framework

## Overview

The `Measurements` class models continuous signals derived from infections. Unlike count observations (hospital admissions, deaths), measurements are continuous values that may span orders of magnitude or even be negative (e.g., log-transformed data).

**Examples of measurement data:**

- Wastewater viral concentrations
- Air quality pathogen levels
- Serological assay results
- Environmental sensor readings

### The general pattern

All measurement observation processes follow the same pattern:

$$\text{observed} \sim \text{Noise}\bigl(\text{predicted}(\text{infections})\bigr)$$

where:

1. **`_predicted_obs(infections)`**: Transforms infections into predicted measurement values (you implement this)
2. **Noise model**: Adds stochastic variation around predictions (provided by PyRenew)

The `Measurements` base class provides:

- Convolution utilities for temporal delays
- Timeline alignment between infections and observations
- Integration with hierarchical noise models
- Support for multiple sensors and subpopulations

### Comparison with count observations

The core convolution structure is shared with count observations, but key aspects differ:

| Aspect | Counts | Measurements |
|--------|--------|--------------|
| Output type | Discrete counts | Continuous values |
| Output space | Linear (expected counts) | Often log-transformed |
| Noise model | Poisson or Negative Binomial | Normal (often on log scale) |
| Scaling | Ascertainment rate $\alpha \in [0,1]$ | Domain-specific |
| Subpop structure | Optional (`CountsBySubpop`) | Inherent (sensor/site effects) |

## The noise model

Measurement data typically exhibits **sensor-level variability**: different instruments, labs, or sampling locations have systematic biases and different precision levels.

`HierarchicalNormalNoise` models this with two per-sensor parameters:

- **Sensor mode**: Systematic bias (additive shift)
- **Sensor SD**: Measurement precision (noise level)

```
observed ~ Normal(predicted + sensor_mode[sensor], sensor_sd[sensor])
```

The noise model samples sensor-level parameters within a plate, so any `RandomVariable` can be used as a prior:

```{python}
# | label: noise-model-general
# Sensor modes: zero-centered, allowing positive or negative bias
sensor_mode_rv = DistributionalVariable("sensor_mode", dist.Normal(0, 0.5))

# Sensor SDs: must be positive, truncated normal is a common choice
sensor_sd_rv = DistributionalVariable(
    "sensor_sd", dist.TruncatedNormal(loc=0.3, scale=0.15, low=0.05)
)

# Create noise model
noise = HierarchicalNormalNoise(
    sensor_mode_rv=sensor_mode_rv,
    sensor_sd_rv=sensor_sd_rv,
)
```

## The indexing system

Measurement observations use three index arrays to map observations to their context:

| Index array | Purpose |
|-------------|---------|
| `times` | Day index for each observation |
| `subpop_indices` | Which infection trajectory (subpopulation) generated each observation |
| `sensor_indices` | Which sensor made each observation (determines noise parameters) |

This flexible indexing supports:

- **Irregular sampling**: Observations don't need to be daily
- **Multiple sensors per subpopulation**: Different labs analyzing the same source
- **Multiple subpopulations per sensor**: One sensor serving multiple areas (less common)

## Subclassing Measurements

To create a measurement process for your domain, subclass `Measurements` and implement:

1. **`_predicted_obs(infections)`**: Transform infections to predicted values
2. **`validate()`**: Check parameter validity
3. **`lookback_days()`**: Return the temporal PMF length

```python
class MyMeasurement(Measurements):
    def __init__(self, temporal_pmf_rv, noise, my_scaling_param):
        super().__init__(temporal_pmf_rv=temporal_pmf_rv, noise=noise)
        self.my_scaling_param = my_scaling_param

    def _predicted_obs(self, infections):
        # Your domain-specific transformation here
        pmf = self.temporal_pmf_rv()
        # ... convolve, scale, transform ...
        return predicted_values

    def validate(self):
        pmf = self.temporal_pmf_rv()
        self._validate_pmf(pmf, "temporal_pmf_rv")

    def lookback_days(self):
        return len(self.temporal_pmf_rv())
```

---

# Part 2: Wastewater Example

We now illustrate the framework with a wastewater viral concentration model, based on the [PyRenew-HEW](https://github.com/cdcgov/pyrenew-hew) family of models.

## The wastewater signal

Wastewater treatment plants measure viral RNA concentrations in sewage. The predicted concentration depends on:

- **Infections**: People shed virus into wastewater
- **Shedding kinetics**: Viral shedding peaks a few days after infection
- **Scaling factors**: Genome copies per infection, wastewater volume

The predicted log-concentration on day $t$ is:

$$\log(\lambda_t) = \log\left(\frac{G}{V} \cdot \sum_{d=0}^{D} I_{t-d} \cdot p_d\right)$$

where:

- $I_{t-d}$ is infections on day $t-d$
- $p_d$ is the shedding kinetics PMF (fraction shed on day $d$ post-infection)
- $G$ is genome copies shed per infection
- $V$ is wastewater volume per person per day

Observations are log-concentrations with normal noise:

$$y_t \sim \text{Normal}(\log(\lambda_t) + \text{sensor\_mode}, \text{sensor\_sd})$$

## Implementing the Wastewater class

```{python}
# | label: wastewater-class
from jax.typing import ArrayLike
from pyrenew.metaclass import RandomVariable
from pyrenew.observation.noise import MeasurementNoise


class Wastewater(Measurements):
    """
    Wastewater viral concentration observation process.

    Transforms site-level infections into predicted log-concentrations
    via shedding kinetics convolution and genome/volume scaling.
    """

    def __init__(
        self,
        shedding_kinetics_rv: RandomVariable,
        log10_genome_per_infection_rv: RandomVariable,
        ml_per_person_per_day: float,
        noise: MeasurementNoise,
    ) -> None:
        """
        Initialize wastewater observation process.

        Parameters
        ----------
        shedding_kinetics_rv : RandomVariable
            Viral shedding PMF (fraction shed each day post-infection).
        log10_genome_per_infection_rv : RandomVariable
            Log10 genome copies shed per infection.
        ml_per_person_per_day : float
            Wastewater volume per person per day (mL).
        noise : MeasurementNoise
            Noise model (e.g., HierarchicalNormalNoise).
        """
        super().__init__(temporal_pmf_rv=shedding_kinetics_rv, noise=noise)
        self.log10_genome_per_infection_rv = log10_genome_per_infection_rv
        self.ml_per_person_per_day = ml_per_person_per_day

    def validate(self) -> None:
        """Validate parameters."""
        shedding_pmf = self.temporal_pmf_rv()
        self._validate_pmf(shedding_pmf, "shedding_kinetics_rv")
        self.noise.validate()

    def lookback_days(self) -> int:
        """Return shedding PMF length."""
        return len(self.temporal_pmf_rv())

    def _predicted_obs(self, infections: ArrayLike) -> ArrayLike:
        """
        Compute predicted log-concentration from infections.

        Applies shedding kinetics convolution, then scales by
        genome copies and volume to get concentration.
        """
        shedding_pmf = self.temporal_pmf_rv()
        log10_genome = self.log10_genome_per_infection_rv()

        # Convolve each site's infections with shedding kinetics
        def convolve_site(site_infections):
            convolved, _ = self._convolve_with_alignment(
                site_infections, shedding_pmf, p_observed=1.0
            )
            return convolved

        # Apply to all subpops (infections shape: n_days x n_subpops)
        shedding_signal = jax.vmap(convolve_site, in_axes=1, out_axes=1)(
            infections
        )

        # Convert to concentration: genomes per mL
        genome_copies = 10**log10_genome
        concentration = (
            shedding_signal * genome_copies / self.ml_per_person_per_day
        )

        # Return log-concentration (what we model)
        return jnp.log(concentration)
```

## Configuring wastewater-specific parameters

### Viral shedding kinetics

The shedding PMF describes what fraction of total viral shedding occurs on each day after infection:

```{python}
# | label: shedding-pmf
# Peak shedding ~3 days after infection, continues for ~10 days
shedding_pmf = jnp.array(
    [0.0, 0.05, 0.15, 0.25, 0.20, 0.15, 0.10, 0.05, 0.03, 0.02]
)
print(f"PMF sums to: {shedding_pmf.sum():.2f}")

shedding_rv = DeterministicPMF("viral_shedding", shedding_pmf)

# Summary statistics
days = np.arange(len(shedding_pmf))
mean_shedding_day = float(np.sum(days * shedding_pmf))
mode_shedding_day = int(np.argmax(shedding_pmf))
print(f"Mode: {mode_shedding_day} days, Mean: {mean_shedding_day:.1f} days")
```

```{python}
# | label: fig-shedding
# | fig-cap: Viral shedding kinetics distribution
fig, ax = plt.subplots(figsize=(8, 4))
ax.bar(days, shedding_pmf, color="steelblue", alpha=0.7, edgecolor="black")
ax.axvline(
    mode_shedding_day,
    color="purple",
    linestyle="-",
    linewidth=2,
    label=f"Mode: {mode_shedding_day}",
)
ax.axvline(
    mean_shedding_day,
    color="red",
    linestyle="--",
    linewidth=2,
    label=f"Mean: {mean_shedding_day:.1f}",
)
ax.set_xlabel("Days after infection")
ax.set_ylabel("Fraction of total shedding")
ax.set_title("Viral Shedding Kinetics")
ax.legend()
plt.tight_layout()
plt.show()
```

### Genome copies and wastewater volume

```{python}
# | label: scaling-params
# Log10 genome copies shed per infection (typical range: 8-10)
log10_genome_rv = DeterministicVariable("log10_genome", 9.0)

# Wastewater volume per person per day (mL)
ml_per_person_per_day = 1000.0
```

### Sensor-level noise

For wastewater, a "sensor" is a WWTP/lab pairâ€”the combination of treatment plant and laboratory that determines measurement characteristics:

```{python}
# | label: ww-noise-model
# Sensor-level mode: systematic differences between WWTP/lab pairs
ww_sensor_mode_rv = DistributionalVariable(
    "ww_sensor_mode", dist.Normal(0, 0.5)
)

# Sensor-level SD: measurement variability within each WWTP/lab pair
ww_sensor_sd_rv = DistributionalVariable(
    "ww_sensor_sd", dist.TruncatedNormal(loc=0.3, scale=0.15, low=0.10)
)

ww_noise = HierarchicalNormalNoise(
    sensor_mode_rv=ww_sensor_mode_rv,
    sensor_sd_rv=ww_sensor_sd_rv,
)
```

## Creating the wastewater observation process

```{python}
# | label: create-process
ww_process = Wastewater(
    shedding_kinetics_rv=shedding_rv,
    log10_genome_per_infection_rv=log10_genome_rv,
    ml_per_person_per_day=ml_per_person_per_day,
    noise=ww_noise,
)

print(f"Required lookback: {ww_process.lookback_days()} days")
```

## Demonstrations

### Timeline alignment

The observation process maintains alignment: day $t$ in output corresponds to day $t$ in input. The first `lookback_days() - 1` days have incomplete history and are marked invalid.

```{python}
# | label: helper-function
def first_valid_observation_day(obs_process) -> int:
    """Return the first day index with complete infection history."""
    return obs_process.lookback_days() - 1
```

### Simulating from an infection spike

To see how infections spread into concentrations via shedding kinetics, we simulate from a single-day spike:

```{python}
# | label: simulate-spike
n_days = 50
day_one = first_valid_observation_day(ww_process)

# Create infections with a spike (shape: n_days x n_subpops)
infection_spike_day = day_one + 10
infections = jnp.zeros((n_days, 1))  # 1 subpopulation
infections = infections.at[infection_spike_day, 0].set(2000.0)

# For plotting
spike_day = infection_spike_day - day_one
n_plot_days = n_days - day_one

# Observation times and indices
observation_days = jnp.arange(day_one, 40, dtype=jnp.int32)
n_obs = len(observation_days)

with numpyro.handlers.seed(rng_seed=42):
    ww_obs = ww_process.sample(
        infections=infections,
        subpop_indices=jnp.zeros(n_obs, dtype=jnp.int32),
        sensor_indices=jnp.zeros(n_obs, dtype=jnp.int32),
        times=observation_days,
        obs=None,
        n_sensors=1,
    )
```

```{python}
# | label: fig-spike-infections
# | fig-cap: Input infections with a single-day spike
fig, ax = plt.subplots(figsize=(8, 4))
ax.plot(
    np.arange(n_plot_days),
    np.array(infections[day_one:, 0]),
    "-o",
    color="darkblue",
)
ax.axvline(spike_day, color="darkred", linestyle="--", alpha=0.7)
ax.annotate(
    f"Infection spike\n(day {spike_day})",
    xy=(spike_day, 1800),
    xytext=(spike_day + 3, 1800),
    fontsize=10,
    color="darkred",
)
ax.set_xlabel("Day")
ax.set_ylabel("Daily Infections")
ax.set_title("Infections (Input)")
plt.tight_layout()
plt.show()
```

### Observation noise

Sampling multiple times from the same infections shows the range of possible observations:

```{python}
# | label: sample-realizations
n_samples = 50
samples_by_day = {int(d - day_one): [] for d in observation_days}

for seed in range(n_samples):
    with numpyro.handlers.seed(rng_seed=seed):
        ww_result = ww_process.sample(
            infections=infections,
            subpop_indices=jnp.zeros(n_obs, dtype=jnp.int32),
            sensor_indices=jnp.zeros(n_obs, dtype=jnp.int32),
            times=observation_days,
            obs=None,
            n_sensors=1,
        )
    for day_idx, conc in zip(observation_days, ww_result.observed):
        samples_by_day[int(day_idx) - day_one].append(float(conc))

# Convert to arrays for plotting
plot_days = sorted(samples_by_day.keys())
all_samples = np.array(
    [[samples_by_day[d][i] for d in plot_days] for i in range(n_samples)]
)
sample_mean = all_samples.mean(axis=0)
```

```{python}
# | label: fig-sampled-concentrations
# | fig-cap: Multiple realizations showing observation noise
fig, ax = plt.subplots(figsize=(8, 4))

# Plot all samples
for i in range(1, n_samples):
    ax.plot(
        plot_days, all_samples[i], color="orange", alpha=0.15, linewidth=0.5
    )

# Highlight one sample
ax.plot(
    plot_days,
    all_samples[0],
    color="steelblue",
    linewidth=1,
    label="One realization",
)

# Sample mean
ax.plot(
    plot_days, sample_mean, color="darkred", linewidth=1.2, label="Sample mean"
)

ax.axvline(spike_day, color="darkblue", linestyle="--", alpha=0.5)
ax.set_xlabel("Day")
ax.set_ylabel("Log Viral Concentration")
ax.set_title(f"Observation Noise: {n_samples} Samples from Same Infections")
ax.legend()
plt.tight_layout()
plt.show()
```

### Sensor-level variability

Different sensors (WWTP/lab pairs) measuring the same underlying infections show systematic differences:

```{python}
# | label: multi-sensor
num_sensors = 4
infections_constant = jnp.ones((40, 1)) * 1500.0

# Each sensor samples at multiple time points
times_per_sensor = 10
observation_times = jnp.tile(
    jnp.arange(10, 30, 2, dtype=jnp.int32), num_sensors
)
sensor_ids = jnp.repeat(
    jnp.arange(num_sensors, dtype=jnp.int32), times_per_sensor
)
subpop_ids = jnp.zeros(num_sensors * times_per_sensor, dtype=jnp.int32)

with numpyro.handlers.seed(rng_seed=42):
    ww_multi_sensor = ww_process.sample(
        infections=infections_constant,
        subpop_indices=subpop_ids,
        sensor_indices=sensor_ids,
        times=observation_times,
        obs=None,
        n_sensors=num_sensors,
    )
```

```{python}
# | label: fig-multi-sensor
# | fig-cap: Multiple sensors observing the same infections
fig, ax = plt.subplots(figsize=(8, 4))

colors = ["steelblue", "coral", "green", "purple"]
for sensor in range(num_sensors):
    mask = np.array(sensor_ids) == sensor
    ax.plot(
        np.array(observation_times)[mask],
        np.array(ww_multi_sensor.observed)[mask],
        "-o",
        color=colors[sensor],
        label=f"Sensor {sensor}",
    )

ax.set_xlabel("Day")
ax.set_ylabel("Log Viral Concentration")
ax.set_title("Multiple Sensors Observing Same Infections")
ax.legend()
plt.tight_layout()
plt.show()
```

Each sensor has a different baseline (sensor mode) and measurement variability (sensor SD). During inference, these sensor-specific effects are learned from data.

### Multiple subpopulations

In hierarchical models, each sensor may serve a distinct catchment area (subpopulation) with different infection dynamics:

```{python}
# | label: multi-subpop
# Two subpopulations with different infection patterns
n_days_mp = 40
infections_subpop1 = 1000.0 * jnp.exp(
    -jnp.arange(n_days_mp) / 20.0
)  # Slow decay
infections_subpop2 = 2000.0 * jnp.exp(
    -jnp.arange(n_days_mp) / 10.0
)  # Fast decay
infections_multi = jnp.stack([infections_subpop1, infections_subpop2], axis=1)

# Two sensors, each observing a different subpopulation
obs_days_mp = jnp.tile(jnp.arange(10, 30, 2, dtype=jnp.int32), 2)
subpop_ids_mp = jnp.array([0] * 10 + [1] * 10, dtype=jnp.int32)
sensor_ids_mp = jnp.array([0] * 10 + [1] * 10, dtype=jnp.int32)

with numpyro.handlers.seed(rng_seed=42):
    ww_multi_subpop = ww_process.sample(
        infections=infections_multi,
        subpop_indices=subpop_ids_mp,
        sensor_indices=sensor_ids_mp,
        times=obs_days_mp,
        obs=None,
        n_sensors=2,
    )
```

```{python}
# | label: fig-multi-subpop
# | fig-cap: Sensors observing different subpopulations
fig, ax = plt.subplots(figsize=(8, 4))

for subpop in [0, 1]:
    mask = np.array(subpop_ids_mp) == subpop
    ax.plot(
        np.array(obs_days_mp)[mask],
        np.array(ww_multi_subpop.observed)[mask],
        "-o",
        label=f"Subpop {subpop}",
    )

ax.set_xlabel("Day")
ax.set_ylabel("Log Viral Concentration")
ax.set_title("Multiple Subpopulations with Different Infection Dynamics")
ax.legend()
plt.tight_layout()
plt.show()
```

---

## Summary

The `Measurements` framework provides:

1. **A consistent interface** for continuous observation processes
2. **Hierarchical noise models** that capture sensor-level variability
3. **Flexible indexing** for irregular, multi-sensor, multi-subpopulation data
4. **Convolution utilities** with proper timeline alignment

To use it for your domain:

1. Subclass `Measurements`
2. Implement `_predicted_obs()` with your signal transformation
3. Configure appropriate priors for sensor-level effects
4. Use the indexing system to map observations to their context
